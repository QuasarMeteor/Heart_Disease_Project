---
title: "Heart Disease Mean Imputation"
author: "Esteban Salazar"
date: "2024-09-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ROCR)
library(randomForest)
library(MASS)
library(e1071)
library(reshape)
library(ggplot2)
```

# Heart Disease Data Mean Imputation 
During data cleaning we encounter missing or NA values. We can replace the
numerical values with the average of each respective feature. For the categorical,
we have to remove the values. 

## Loading the data
```{r}
disease <- read.csv("C:/Users/esteb/iCloudDrive/Downloads/heartdisease1 copy.csv")
attach(disease)
```

The UCI Machine Learning Repository provides us with the following:

  * 1(age), 4(trestbps), 5(chol), 8(thalach), 10(oldpeak), and 12(ca) are numerical
  * 3(cp), 7(restecg), 11(slope), 13(thal) are categorical
  * 2(sex), 6(fbs), 9(exang), and 14(heart disease) are binary
  
We can see that we have `r nrow(disease)` total observations, however in mean
imputation we will impute the numerical values and remove the categorical. This
means we will have a smaller data set. 
  
```{r}
# Numerical
disease$age[is.na(disease$age)] <- mean(disease$age,na.rm=TRUE) # 1
disease$trestbps[is.na(disease$trestbps)] <- mean(disease$trestbps,na.rm=TRUE) # 4
disease$chol[is.na(disease$chol)] <- mean(disease$chol,na.rm=TRUE) # 5
disease$thalach[is.na(disease$thalach)] <- mean(disease$thalach,na.rm=TRUE) # 8
disease$oldpeak[is.na(disease$oldpeak)] <- mean(disease$oldpeak,na.rm=TRUE) # 10
disease$ca[is.na(disease$ca)] <- round(mean(disease$ca,na.rm=TRUE), digits = 0) #12
```


```{r}
# Omit the Missing Values
dim(na.omit(disease))
disease <- na.omit(disease)
```
Now the size is significantly smaller `r nrow(disease)`

```{r}
# Categorical OR Binary
disease[,3] <- as.factor(disease[,3])
disease[,7] <- as.factor(disease[,7])
disease[,11] <- as.factor(disease[,11])
disease[,13] <- as.factor(disease[,13])
disease[,2] <- as.factor(disease[,2])
disease[,6] <- as.factor(disease[,6])
disease[,9] <- as.factor(disease[,9])
disease[,14] <- as.factor(disease[,14])
```

```{r}
levels(disease[,3]) # 4 Levels (1-4)
levels(disease[,7]) # 3 Levels (0-3)
levels(disease[,11]) # 3 Levels (1-3)
levels(disease[,13]) # 3 Levels (3, 6, 7)
levels(disease[,2]) # 2 Levels 0 or 1
levels(disease[,6]) # 2 Levels 0 or 1
levels(disease[,9]) # 2 Levels 0 or 1
```

## Exploratory Data Analysis
```{r}
par(mfrow = c(2,3))
xlabel = "Heart Disease"
boxplot(age~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Heart Disease by age", xlab = xlabel)
boxplot(trestbps~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Hear Disease by trestbps", xlab = xlabel) # Kinda different
boxplot(chol~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Heart Disease by chol", xlab = xlabel) # Not very different
boxplot(thalach~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Heart Disease by thalach", xlab = xlabel)
boxplot(oldpeak~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Heart Disease by oldpeak", xlab = xlabel)
boxplot(ca~heartdisease, data = disease, col = c("darkgreen", "darkred"), main = "Heart Disease by ca", xlab = xlabel)
```

```{r}
# Barplot by Sex
colors = c('pink', 'lightblue')
ggplot(disease, aes(x=heartdisease, fill = sex)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("Female: 0", "Male: 1"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by Sex") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )

```

```{r}
# Barplot by Chest Pain cp
colors = c('darkgreen', 'green', 'grey', 'red')
ggplot(disease, aes(x=heartdisease, fill = cp)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("Typical: 1", "Atypical: 2", "Non-anginal: 3", "Asymptomatic: 4"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by Chest Pain") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```

```{r}
colors = c('black', 'grey')
ggplot(disease, aes(x=heartdisease, fill = fbs)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("False: 0", "True: 1"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by High Fasting Blood Sugar") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```
```{r}
colors = c('green', 'grey', 'red')
ggplot(disease, aes(x=heartdisease, fill = restecg)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("Normal: 0", "Abnormal: 1", "Definite Hypertrophy: 2"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by ECG") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```

```{r}
colors = c('black','grey')
ggplot(disease, aes(x=heartdisease, fill = exang)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("No: 0", "Yes: 1"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by Excerise Induced Angina") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```

```{r}
colors = c('green', 'grey', 'red')
ggplot(disease, aes(x=heartdisease, fill = slope)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("Upsloping: 1", "Flat: 2", "Downsloping: 3"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by Slope of Peak Excercise") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```

```{r}
colors = c('green', 'grey', 'red')
ggplot(disease, aes(x=heartdisease, fill = thal)) +
  geom_bar(position = 'dodge', width = 0.5, alpha = 0.7) +
  scale_fill_manual(label = c("Normal: 3", "Fixed Defect: 6", "Reversable Defect: 7"),values=colors) +
  labs(x= 'Heart Disease', y= "Frequency") +
  ggtitle("Presence of Heart Disease by Chest Pain") +
  theme_minimal() +
  theme(
    plot.title = element_text(size=18, face='bold'),
    axis.text = element_text(size=12),
    axis.title = element_text(size=14, face='bold'),
    legend.title = element_text(size=12)
  )
```

```{r}
# Correlation Matrix
cormat <- round(cor(disease[,-c(2,3,6,7,9,11,13, 14)]),4)

# Remove the redundant data from lower triangle
upper_tri <- function(cormat){
  cormat[lower.tri(cormat)] <- NA
  return(cormat)
}

# Making the heatmap
upper_triangle <- upper_tri(cormat)
library(reshape2)
melted_cormat <- melt(upper_triangle, na.rm = TRUE)
heatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill=value)) +
  geom_tile(color="white")+
  scale_fill_gradient2(low = 'red', high = 'green', mid='white',
    midpoint = 0, limit=c(-1,1), space = 'Lab', name = "Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45, vjust=1, size=12, hjust = 1))+
  coord_fixed()

# Adding the correlation coefficients
heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position.inside = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

```
Most of these do not show very strong correlations with one another, but maybe several pairs would be worth looking into. 

## Data Modeling

Now split the data into training and testing sets, using a 70-30 split. 
```{r}
set.seed(2024)
samp <- sample(c(TRUE, FALSE), nrow(disease), replace = TRUE, prob = c(0.7,0.3))

train <- disease[samp,]
test <- disease[!samp,]
```


### Logistic Regression
```{r}
set.seed(2024)
# Build the model
log.model <- glm(heartdisease~., data = train, family = binomial)
```

```{r}
MASS::stepAIC(log.model, direction = "backward")
```
AIC 95.29 (no thal, thalach, oldpeak, chol, exang).
This means that backwards stepwise AIC has trestbps, fbs, age, slope, restecg, ca, cp, and sex

```{r}
# Final model
log.model <- glm(heartdisease~age+sex+cp+trestbps+fbs+restecg+slope+ca, data = train, family = binomial)
```

```{r}
# Predicted probability of being in class
log.pred <- predict(log.model, newdata = test, type = "response") 
log.class <- as.numeric(log.pred > 0.5)
# Confusion Matrix
(mat <- xtabs(~log.class + test$heartdisease))

# Error rate
(mat[1,2] + mat[2,1])/sum(mat)
```
A error rate of about 0.1786

### ROC curve and AUC 
```{r}
set.seed(1)
log.pred <- predict(log.model, newdata = test)
pred <- prediction(predictions = log.pred, labels = test$heartdisease)

log.roc <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(log.roc, main = "Logistic ROC")
log_perf2 <- performance(pred, measure = "auc")
log_perf2@y.values[[1]]
```
The AUC is about 0.8310. 

### Random Forest

```{r}
p <- ncol(train) - 1
k <- 5
ntree <- c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)

n <- 1:p 
tuningpar <- expand.grid(n, ntree)
cv.er.forest <- matrix(nrow = k, ncol = nrow(tuningpar))
```


### Cross Validation K-Fold Split
```{r}
set.seed(1)
obs.per.fold <- ceiling(nrow(train)/k)
shuffle.indices <- sample(nrow(train), nrow(train))
folds <- vector("list", length = k)
for (i in 1:k) {
  if (i != k) { 
    fold.indices <- (i - 1)*obs.per.fold + 1:obs.per.fold
    folds[[i]] <- shuffle.indices[fold.indices]
  } else {
    fold.indices <- ((i - 1)*obs.per.fold + 1):nrow(train)
    folds[[i]] <- shuffle.indices[fold.indices]
  }
}
```


### Testing for the two hyper parameters, ntrees && mtry.
```{r}
set.seed(1)
for (i in 1:k) {
  cv.training <- train[-folds[[i]],]
  cv.validation <- train[folds[[i]],]
    for (j in 1:nrow(tuningpar)) {
      rf <- randomForest(x = cv.training[,-14], y = cv.training[,14], mtry = tuningpar[j,1],
      ntree = tuningpar[j,2])
      rfpred <- predict(rf, newdata = cv.validation[,-14])
      cv.er.forest[i,j] <- mean(rfpred != cv.validation[,14])
    }
}
meancv <- colMeans(cv.er.forest)
optimal <- tuningpar[which(meancv == min(meancv)),]
names(optimal) <- c("# of Parameters", "# of Trees")
optimal
```
Found that 200 trees and 11 mtry parameters optimizes the randomforest model. 

### Prediction based on the test case
```{r}
rand.forest <- randomForest(x = train[,-14], y = train[,14], mtry = 11, ntree = 200)

#Predicted probability of being in class
rand.pred <- predict(rand.forest, newdata = test[,-14], type = "prob")[,2]
```

### Importance
```{r}
varImpPlot(rand.forest, main = "Importance of Predictors")
```
We can see that by the gini index, the cp has the highest importance, followed by thalach then thal 

### Random Forest Confusion Matrix
```{r}
rand.class <- predict(rand.forest, newdata = test[,-14], type = "response")
(mat <- xtabs(~rand.class + test$heartdisease))
(mat[1,2]+mat[2,1])/sum(mat) # Error rate
```
The test error rate is about 23.21%

### ROC and AUC Curves
```{r}
set.seed(1)
pred <- prediction(predictions = rand.pred, labels = test$heartdisease)

rand.roc <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(rand.roc, main = "Random Forest ROC")
log_perf2 <- performance(pred, measure = "auc")
log_perf2@y.values[[1]]
```
The area under the curve is about 86.71%

### Chosen Method) SVM

```{r}
set.seed(2024)
tune.out <- tune(svm, heartdisease~., data = train, kernel = "polynomial",ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100), degree = c(1,2,3,4)))
summary(tune.out)
best <- summary(tune.out)$best.model
```

```{r}
svm.pred <- predict(best, newdata = test)
(mat <- xtabs(~svm.pred + test$heartdisease))
(mat[1,2] + mat[2,1])/sum(mat)
```
About 12.50%

### ROC
```{r}
#Create the function
rocplot<-function(predi,truth,...){
predob<-prediction(predi,truth)
perf<-performance(predob,"tpr","fpr")
plot(perf,...)
}
```

```{r}
# Obtain the fitted values
svm.fit <- svm(heartdisease~., data = train, kernel = "polynomial", degree = 1, cost = 1, decision.values=TRUE)
fitted <- attributes(predict(svm.fit, newdata = test, decision.values=TRUE))$decision.values
rocplot(-fitted, test$heartdisease, main = "Support Vector Machine (Poly) ROC")
```


### AUC
```{r}
svm.fit <- svm(heartdisease~., data = train, kernel = "polynomial", degree = 1, cost = 1, probability=TRUE)
prob <- attributes(predict(svm.fit, newdata = test, probability =TRUE))$probabilities[,2]

predictionsvm <- prediction(prob,test$heartdisease)
svm_perf2 <- performance(predictionsvm, measure = "auc")
svm_perf2@y.values[[1]]
```